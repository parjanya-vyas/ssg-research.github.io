---
layout: default
permalink: /mlsec/modelExtDef
nav: false
horizontal: false
---

<!doctype html>

<style>
    .bg-lighter{
        background-color: #f8f9fa;
    }
</style>
<body>

<h1>Model Extraction Attacks and Defenses</h1>	

<h2>Background</h2>  
<p>As machine learning (ML) applications become increasingly prevalent, protecting the confidentiality of ML models becomes paramount. One way to protect model confidentiality is to limit access to the model only via well-defined prediction APIs. Nevertheless, prediction APIs still leak information so that it is possible to mount model extraction attacks. In model extraction, the adversary only has access to the prediction API of a target model which he queries to extract information about the model internals. The adversary uses this information to gradually train a substitute model that reproduces the predictive behaviour of the target model.</p>
<h2>Conference/journal paper publications</h2>
<ul>
<li>Asim Waheed, Vasisht Duddu, N. Asokan: <strong>GrOVe: Ownership Verification of Graph Neural Networks using Embeddings.</strong> <a href="https://sp2024.ieee-security.org/">IEEE S&amp;P 2024</a>. arXiv preprint <a href="https://arxiv.org/abs/2304.08566">arXiv:2304.08566</a></li>
<li>Buse Gul Atli Tekgul, N Asokan: <strong>FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks.</strong> <a href="https://sp2024.ieee-security.org/">ACSAC 2023</a>. arXiv preprint <a href="https://arxiv.org/abs/2307.14751">arXiv:2307.14751</a></li>
<li>Sebastian Szyller, Rui Zhang, Jian Liu, N. Asokan: <strong>On the Robustness of Dataset Inference.</strong> <a href="https://openreview.net/forum?id=LKz5SqIXPJ">TMLR 2023</a>. arXiv preprint <a href="https://arxiv.org/abs/2210.13631">arXiv:2210.13631</a></li>
<li>Sebastian Szyller, N. Asokan: <strong>Conflicting Interactions Among Protection Mechanisms for Machine Learning Models</strong>. <a href="https://aaai-23.aaai.org">AAAI 2023</a>. arXiv preprint <a href="https://arxiv.org/abs/2207.01991">arXiv:2207.01991</a></li>
<li>Buse G. A. Tekgul, N. Asokan:<strong> On the Effectiveness of Dataset Watermarking.  </strong><a href="https://sites.google.com/view/iwspa-2022/">CODASPY-IWSPA 2022</a>. arXiv preprint <a href="https://arxiv.org/abs/2202.12506">arXiv:2202.12506</a></li>
<li>Sebastian Szyller, Buse G. A. Tekgul, Samuel Marchal, N. Asokan: <strong>DAWN: Dynamic Adversarial Watermarking of Neural Networks. </strong><a href="https://2021.acmmm.org/">ACM Multimedia 2021</a>. arXiv preprint <a href="https://arxiv.org/abs/1906.00830">arXiv:1906.00830</a></li>
<li>Buse G. A. Tekgul, Yuxi Xia, Samuel Marchal, N. Asokan. <strong>WAFFLE: Watermarking in Federated Learning. </strong><a href="https://srds-conference.org/">SRDS 2021</a>. arXiv preprint <a href="https://arxiv.org/abs/2008.07298">arXiv:2008.07298</a></li>
<li>Buse G. A. Tekgul, Sebastian Szyller, Mika Juuti, Samuel Marchal, N. Asokan: <strong>Extraction of Complex DNN Models: Real Threat or Boogeyman? </strong><a href="https://sites.google.com/view/edsmls2020/home">AAAI-EDSMLS 2020</a>. arXiv preprint: <a href="https://arxiv.org/abs/1910.05429">arXiv:1910.05429 [cs.LG] </a></li>
<li>Mika Juuti, Sebastian Szyller, Alexey Dmitrenko, Samuel Marchal, N. Asokan: <strong>PRADA: Protecting against DNN Model Stealing Attacks. </strong>IEEE Euro S&amp;P 2019. arXiv preprint <a href="https://arxiv.org/abs/1805.02628">arXiv:1805.02628</a></li>
</ul>
<h2>Technical reports</h2>
<ul>
<li>Rui Zhang, Jian Liu, Sebastian Szyller, Kui Ren, N. Asokan: <strong>False Claims Against Model Ownership Resolution.</strong> arXiv preprint <a href="https://arxiv.org/abs/2304.06607">arXiv:2304.06607</a></li>
<li>Sebastian Szyller, Vasisht Duddu, Tommi Gr√∂ndahl, N. Asokan: <strong>Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks.  </strong>arXiv preprint <a href="https://arxiv.org/abs/2104.12623">arXiv:2104.12623</a></li>
</ul>
<h2>Theses</h2>
<ul>
<li>Sebastian Szyller (PhD Dissertation): <strong>Ownership and Confidentiality in Machine Learning</strong> (2023), <a href="https://aaltodoc.aalto.fi/handle/123456789/122309">link</a></li>
<li>Asim Waheed (MMath Thesis @UWaterloo): <strong>On Using Embeddings for Ownership Verification of Graph Neural Networks</strong> (2023), <a href="https://uwspace.uwaterloo.ca/handle/10012/19674">link</a></li>
<li>Buse Gul Atli (PhD Dissertation): <strong>Securing Machine Learning: Streamlining Attacks and Defenses Under Realistic Adversary Models</strong> (2022), <a href="https://aaltodoc.aalto.fi/handle/123456789/115558">link</a></li>
<li>Shelly Wang (MMath Thesis @UWaterloo): <strong>Security and Ownership Verification in Deep Reinforcement Learning</strong> (2022), <a href="https://uwspace.uwaterloo.ca/handle/10012/18443">link</a></li>

</ul>
<h2>Talks</h2>
<ul>
<li>On the Effectiveness of Dataset Watermarking: IWSPA&#8217;22 talk [<a href="/wp-content/uploads/2022/05/IWSPA2022-DatasetWatermarking.pdf">pdf</a>]</li>
<li>WAFFLE: SRDS&#8217;21 talk [<a href="/wp-content/uploads/2022/05/SRDS2021-WAFFLE.pdf">pdf</a>]</li>
<li>Extraction of Complex DNN models: Brief overview [<a href="/wp-content/uploads/2020/02/IntelTalk_Jan_2020.pdf">pdf</a>], AAAI-EDSMLS&#8217;20 talk [<a href="/wp-content/uploads/2019/12/EDSMLS_presentation.pdf">pdf</a>]</li>
<li>PRADA: Euro S&amp;P&#8217;19 talk [<a href="/wp-content/uploads/2019/08/EuroSP19.pdf">pdf</a>]</li>
</ul>
<h2>Demos &amp; Posters</h2>
<ul>
<li><a href="https://www.aalto.fi/en/events/cs-research-day-2020">CS Research Day 2020:</a> <strong>WAFFLE: Watermarking in Federated Learning</strong> (October 1, Aalto University, Finland), <a href="https://www.youtube.com/watch?v=C_zoHSU1wOs">presentation</a></li>
<li><a href="https://ssg.aalto.fi/events/demo-day-2019/">Secure Systems Demo Day 2019</a>: <strong>Stealing Complex DNN Models: Limitations and Defense Strategies</strong> (May 29, Aalto University, Finland), <a href="/wp-content/uploads/2019/05/20_Atli.pdf">poster</a></li>
</ul>
<h2>Source code</h2>
<ul>
<li><a href="https://github.com/ssg-research/GrOVe">GitHub source code for GrOVe</a></li>
<li><a href="https://github.com/ssg-research/FLARE">GitHub source code for FLARE</a></li>
<li><a href="https://github.com/ssg-research/conflicts-in-ml-protection-mechanisms">GitHub source code for Conflicting Interactions</a></li>
<li><a href="https://github.com/ssg-research/WAFFLE">GitHub source code for WAFFLE</a></li>
<li><a href="https://github.com/ssg-research/dawn-dynamic-adversarial-watermarking-of-neural-networks">GitHub source code for DAWN</a></li>
<li><a href="https://github.com/SSGAalto/prada-protecting-against-dnn-model-stealing-attacks">GitHub source code for PRADA</a></li>
</ul>


  
</body>
